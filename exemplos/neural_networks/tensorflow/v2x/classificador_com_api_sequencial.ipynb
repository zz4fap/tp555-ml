{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2604f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57557cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c88dfb",
   "metadata": {},
   "source": [
    "### Carregando o conjunto de dados fashion MNIST. \n",
    "\n",
    "O Keras tem várias funções para carregar conjuntos de dados populares em `keras.datasets`. O conjunto de dados já está dividido para você entre um conjunto de treinamento e um conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddccfd",
   "metadata": {},
   "source": [
    "Vamos dividir o conjunto de treinamento em um conjunto de validação e um conjunto de treinamento (menor). Também **escalonamos** as magnitudes dos pixels para o intervalo de 0-1 e as convertemos em floats, dividindo por 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14530c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384d49d",
   "metadata": {},
   "source": [
    "Plotando algumas imagens do banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb493a79",
   "metadata": {},
   "source": [
    "### Criando um modelo sequencial\n",
    "\n",
    "O modelo abaixo é o de um classificador MLP com duas camadas ocultas. Na sequência, descrevemos cada uma das linhas de código da célula abaixo.\n",
    "\n",
    "+ A primeira linha cria um modelo Sequencial.\n",
    "+ A primeira cadamda é uma camada `Flatten` cuja função é simplesmente converter cada imagem de entrada em uma array 1D. Esta camada não possui parâmetros, é apenas faz um pré-processamento simples de conversão.\n",
    "    + Por ser a primeira camada do modelo, devemos especificar o parâmetro `input_shape`. Ele não inclui o tamanho do mini-batch, apenas as dimensões dos exemplos de entrada.\n",
    "+ Em seguida, adicionamos uma camada oculta densa (i.e., classe `Dense`) com 300 neurônios. Ele usará a função de ativação **ReLU**.\n",
    "+ Em seguida, adicionamos uma segunda camada oculta densa com 100 neurônios, também usando a função de ativação **ReLU**.\n",
    "+ Por fim, adicionamos uma camada de saída densa com 10 neurônios (um para cada classe), usando o função de ativação **softmax**.\n",
    "\n",
    "**IMPORTANTE**\n",
    "\n",
    "+ Outras funções de ativação estão disponíveis no pacote `keras.activations`. Veja a lista completa em https://keras.io/activations/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca235c",
   "metadata": {},
   "source": [
    "### Resumo do modelo\n",
    "\n",
    "O método `summary()` da classe `Sequential` exibe todas as camadas do modelo, incluindo o nome de cada camada (que é gerado automaticamente, a menos que você o defina ao criar a camada), seu formato de saída (`None` significa que o tamanho do mini-batch pode ser qualquer um) e seu número de parâmetros. O resumo termina imprimindo o número total de parâmetros, incluindo treináveis e não treináveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995bf6b",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "+ Observem que as camadas densas geralmente têm muitos parâmetros. Por exemplo, a primeira camada oculta tem 784 x 300 pesos, mais 300 termos de bias, o que soma 235.500 parâmetros.\n",
    "\n",
    "+ Isso dá ao modelo bastante flexibilidade para ajustar os dados de treinamento, mas também significa que o modelo corre o risco de **overfitting**, especialmente quando não temos muitos dados de treinamento.\n",
    "\n",
    "+ As camadas densas têm seus pesos inicializados aleatoriamente e os termos de bias são inicializados com zeros. \n",
    "    + Se quisermos usar um método de inicialização diferente, podemos definir o parâmetro `kernel_initializer` (kernel é outro nome para a matriz de pesos de conexão) ou `bias_initializer` ao criar a camada.\n",
    "    + Os vários métodos de incialização estão listados em: https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c945f2",
   "metadata": {},
   "source": [
    "### Compilando o modelo\n",
    "\n",
    "+ Depois que um modelo é criado, devemos chamar seu método `compile()` para especificar a **função de custo** e o **otimizador** que devem ser usados. \n",
    "\n",
    "+ Opcionalmente, também podemos especificar uma lista de métricas extras para calcular durante o treinamento e a valiação do modelo.\n",
    "\n",
    "+ Listas com outras funções de custo, otimizadores e métricas podem ser encontaradas em\n",
    "\n",
    "    + https://keras.io/losses/,\n",
    "\n",
    "    + https://keras.io/optimizers/ \n",
    "\n",
    "    + https://keras.io/metrics/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b785abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105639db",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "+ Nós usamos a função de custo `sparse_categorical_crossentropy` porque temos rótulos esparsos (ou seja, para cada exemplo há apenas um índice de classe, de 0 a 9 neste caso).\n",
    "+ Em relação ao otimizador, `sgd` significa simplesmente que treinaremos o modelo usando o gradiente descendente estocástico.\n",
    "    + Quando usamos apenas a string com o nome do otimizador, usamos todos os parâmetros com valor padrão (i.e., `learning_rate=0.01`, `momentum=0.0`, `nesterov=False`).\n",
    "+ Como este é o modelo de um classificador, é útil medir sua acurácia (`accuracy`) durante seu treinamento e valiação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dd006",
   "metadata": {},
   "source": [
    "### Treinando e avaliando o modelo\n",
    "\n",
    "+ Para treinar o modelo, usamos o método `fit()`.\n",
    "+ Passamos para ele os atributos de entrada (X_train) e os rótulos (y_train), bem como o número de épocas para treinar. \n",
    "+ Também passamos um conjunto de validação (opcional) que é usado para medir o cutso e as métricas extras neste conjunto no final de cada época.\n",
    "    + Se o desempenho no conjunto de treinamento é muito melhor do que no conjunto de validação, o modelo provavelmente está sobreajustando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de139e6d",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "+ A cada época durante o treinamento, o modelo exibe o número de exemplos processados até o momento (junto com uma barra de progresso), o tempo médio de treinamento por exemplo, a perda e a acurácia (ou qualquer outra métrica extra solicitada), tanto no conjunto de treinamento quanto no conjunto de validação. \n",
    "\n",
    "+ Podemos ver que a perda de treinamento diminuiu, o que é um bom sinal, e a acurácia da validação atingiu 87.28% após 50 épocas, não muito longe da acurácia de treinamento, então não parece não estar correndo **sobreajuste**.\n",
    "\n",
    "+ Em vez de passar um conjunto de validação usando o parâmetro `validation_data`, podemos definir o parâmetro `validation_split` com a proporção do conjunto de treinamento que desejamos que se seja usado para validação (por exemplo, 0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d362f",
   "metadata": {},
   "source": [
    "### Avaliando o modelo\n",
    "\n",
    "+ O método `fit()` retorna um objeto do tipo `History` que contém, além de outros parâmetros, um dicionário (`history.history`) contendo a perda e as métricas extras medidas ao final de cada época no conjunto de treinamento e no conjunto de validação (se houver). \n",
    "\n",
    "+ Podemos criar um `DataFrame` do Pandas usando este dicionário e chamar seu método `plot()`, para plotar as curvas de loss e acurácia mostradas na figura abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cea5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b03ebd",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "+ Percebemos que as acurácias de treinamento e de validação aumentam constantemente durante o treinamento, enquanto as perdas de treinamento e validação diminuem. \n",
    "\n",
    "+ Além disso, as curvas de validação são bastante próximas das curvas de treinamento, o que significa que não há muito sobreajuste. \n",
    "\n",
    "+ Podemos dizer que o modelo ainda não convergiu completamente, pois a perda de validação ainda está diminuindo (não convergiu), então nós provavelmente devemos continuar o treinamento.\n",
    "    + Para continuar o treinamento, basta chamar o método `fit()` novamente, já que Keras continua treinando de onde parou.\n",
    "    + A acuácia no conjunto de validação deve atingir cerca de 89% com aproximadamente 50 épocas.\n",
    "    \n",
    "+ Se não estivermos satisfeitos com o desempenho do modelo, devemos ajustar os hiperparâmetros do modelo, por exemplo, o número de camadas, o número de neurônios por camada, os tipos de funções de ativação que usamos para cada camada oculta, o número das épocas de treinamento, o tamanho do mini-batch.\n",
    "    + O tamanho do mini-batch pode ser definido no método `fit()` usando o argumento `batch_size`, cujo valor padrão é 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648e716",
   "metadata": {},
   "source": [
    "### Fazendo predições com o modelo treinado\n",
    "\n",
    "+ Usamos o método `predict()` do modelo para fazer previsões em novos exemplos. \n",
    "    + Como não temos novos exemplos, usaremos apenas os 3 primeiros exemplos do conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed48ee9",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "+ Como pode ser visto, para cada exemplo o modelo estima uma probabilidade por classe, da classe 0 à classe 9. \n",
    "\n",
    "+ Por exemplo, para a primeira imagem ele estima que a probabilidade da classe 9 (`Ankle boot`) é de 79%, a probabilidade da classe 7 (`Sneaker`) é de 12%, a probabilidade da classe 5 (`Sandal`) é de 9%, e as demais classes têm probabilidade desprezível (`round(2)`). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
