{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a6bb3fdf",
      "metadata": {
        "id": "a6bb3fdf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "996a75c1",
      "metadata": {
        "id": "996a75c1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04f77bc",
      "metadata": {
        "id": "b04f77bc"
      },
      "source": [
        "### Carregando o conjunto de dados para regressão.\n",
        "\n",
        "+ Vamos usar o conjunto de dados habitacional da Califórnia e criar um regressor com uma rede neural.\n",
        "\n",
        "+ Depois de carregar os dados, dividimos em um conjunto de treinamento, um conjunto de validação e um conjunto de teste, e padronizamos todos os atributos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "047f3fc4",
      "metadata": {
        "id": "047f3fc4"
      },
      "outputs": [],
      "source": [
        "# Baixa a base de dados.\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Divide o conjunto total de exemplos em conjuntos de treinamento e teste.\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "\n",
        "# Divide o conjunto de treinamento em conjuntos de treinamento (menor) e validação.\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "# Aplica padronização às matrizes de atributos.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ed5dc5",
      "metadata": {
        "id": "a6ed5dc5"
      },
      "source": [
        "### Criando e compilando o modelo.\n",
        "\n",
        "+ Neste exemplo usaremos a API sequencial, mas as `callbacks` também funcionam com a API funcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3523b1d9",
      "metadata": {
        "id": "3523b1d9"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential(\n",
        "    [\n",
        "        keras.layers.Input(shape=[8]),\n",
        "        keras.layers.Dense(30, activation=\"relu\", name='hidden1'),\n",
        "        keras.layers.Dense(30, activation=\"relu\", name='hidden2'),\n",
        "        keras.layers.Dense(1, name='output')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff704f73",
      "metadata": {
        "id": "ff704f73"
      },
      "source": [
        "### Criando a callback `ModelCheckpoint`.\n",
        "\n",
        "+ A callback `ModelCheckpoint` **salva pontos de verificação do modelo em intervalos regulares** durante o treinamento, por **padrão ao final de cada época**.\n",
        "    + O parâmetro `save_freq`, pode ser configurado com a string `'epoch'` ou um valor inteiro. Ao usar `'epoch'`, a callback salva o modelo após cada época.\n",
        "    + Ao usar um valor inteiro, a callback salva o modelo ao final do número de mini-batches especificado.\n",
        "\n",
        "\n",
        "+ Se usarmos um **conjunto de validação** durante o treinamento, podemos definir o parâmetro `save_best_only=True` ao criar a callback `ModelCheckpoint`.\n",
        "    + Nesse caso, a callback só salvará o modelo quando seu desempenho no conjunto de validação for o melhor até o momento.\n",
        "\n",
        "\n",
        "+ Dessa forma, não precisamos nos preocupar em treinar por muito tempo e sobreajustar ao conjunto de treinamento: basta restaurar o último modelo salvo após o treinamento e este será o melhor modelo no conjunto de validação.\n",
        "    + Precisamos restaurar o modelo manualmente após o treinamento.\n",
        "\n",
        "\n",
        "+ Esta é uma maneira simples de implementar a parada antecipada (i.e., *early-stop*).\n",
        "    + Entretanto, o treinamento não é encerrado antecipadamente, ele ocorre até a última época definida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "08e396d4",
      "metadata": {
        "id": "08e396d4"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"./my_keras_model.keras\", save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7944282",
      "metadata": {
        "id": "f7944282"
      },
      "source": [
        "### Treinando e avaliando o modelo salvo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0cac6102",
      "metadata": {
        "id": "0cac6102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de7a6a3-6b06-4d90-9cea-8afd268ca178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 3.8979 - val_loss: 3.1561\n",
            "Epoch 2/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.8131 - val_loss: 1.1498\n",
            "Epoch 3/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.6668 - val_loss: 0.6078\n",
            "Epoch 4/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6140 - val_loss: 0.5340\n",
            "Epoch 5/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.5795 - val_loss: 0.5076\n",
            "Epoch 6/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.5526 - val_loss: 0.4884\n",
            "Epoch 7/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.5307 - val_loss: 0.4714\n",
            "Epoch 8/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.5124 - val_loss: 0.4572\n",
            "Epoch 9/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.4967 - val_loss: 0.4444\n",
            "Epoch 10/10\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.4834 - val_loss: 0.4326\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4486\n"
          ]
        }
      ],
      "source": [
        "# Treina o modelo com a callback especificada.\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb]\n",
        "                   )\n",
        "\n",
        "# Carrega o melhor modelo no conjunto de validação.\n",
        "model = keras.models.load_model(\"./my_keras_model.keras\")\n",
        "\n",
        "# Avalia o modelo.\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a66d77",
      "metadata": {
        "id": "66a66d77"
      },
      "source": [
        "### Criando a callback `EarlyStopping`.\n",
        "\n",
        "+ Outra maneira de implementar a **parada antecipada** é simplesmente usar a callback `EarlyStopping`.\n",
        "\n",
        "\n",
        "+ **Ela interromperá o treinamento** quando não medir nenhum progresso no conjunto de validação por várias épocas (definidas pelo parâmetro `patience`) e, **opcionalmente, carrega o melhor modelo ao final do treinamento**.\n",
        "    + Por padrão, a métrica avaliada para encerrar o treinamento é a perda no conjunto de validação.\n",
        "\n",
        "\n",
        "+ Podemos combinar as duas callbacks para salvar os pontos de verificação do modelo com a `ModelCheckpoint` (no caso de seu computador travar) e interromper o treinamento mais cedo quando não houver mais progresso, com a `EarlyStopping` (para evitar desperdício de tempo e recursos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6c5b1ad3",
      "metadata": {
        "id": "6c5b1ad3"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa04747",
      "metadata": {
        "id": "0fa04747"
      },
      "source": [
        "### Treinando e avaliando o modelo.\n",
        "\n",
        "+ O número de épocas pode ser definido como um **valor grande**, **pois o treinamento será interrompido automaticamente quando não houver mais progresso**.\n",
        "\n",
        "\n",
        "+ Além disso, não há necessidade de restaurar o melhor modelo neste caso, pois a callback `EarlyStopping` armazenará os melhores pesos (que resultaram na menor perda de validação) e os restaurará ao final do treinamento quando `restore_best_weights=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "43bbe8ca",
      "metadata": {
        "id": "43bbe8ca",
        "outputId": "e0115f9f-caa5-45b0-9a33-127faa2758fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4719 - val_loss: 0.4229\n",
            "Epoch 2/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4619 - val_loss: 0.4143\n",
            "Epoch 3/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4532 - val_loss: 0.4069\n",
            "Epoch 4/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4456 - val_loss: 0.4005\n",
            "Epoch 5/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4390 - val_loss: 0.3947\n",
            "Epoch 6/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4331 - val_loss: 0.3898\n",
            "Epoch 7/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4279 - val_loss: 0.3855\n",
            "Epoch 8/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4232 - val_loss: 0.3816\n",
            "Epoch 9/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4190 - val_loss: 0.3781\n",
            "Epoch 10/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4151 - val_loss: 0.3747\n",
            "Epoch 11/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4115 - val_loss: 0.3718\n",
            "Epoch 12/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4082 - val_loss: 0.3689\n",
            "Epoch 13/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4052 - val_loss: 0.3661\n",
            "Epoch 14/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4023 - val_loss: 0.3636\n",
            "Epoch 15/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3997 - val_loss: 0.3614\n",
            "Epoch 16/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3973 - val_loss: 0.3594\n",
            "Epoch 17/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3950 - val_loss: 0.3576\n",
            "Epoch 18/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3928 - val_loss: 0.3558\n",
            "Epoch 19/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3908 - val_loss: 0.3542\n",
            "Epoch 20/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3889 - val_loss: 0.3526\n",
            "Epoch 21/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3870 - val_loss: 0.3511\n",
            "Epoch 22/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3853 - val_loss: 0.3498\n",
            "Epoch 23/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3837 - val_loss: 0.3484\n",
            "Epoch 24/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3821 - val_loss: 0.3472\n",
            "Epoch 25/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3806 - val_loss: 0.3461\n",
            "Epoch 26/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3792 - val_loss: 0.3449\n",
            "Epoch 27/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.3779 - val_loss: 0.3440\n",
            "Epoch 28/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3766 - val_loss: 0.3430\n",
            "Epoch 29/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3754 - val_loss: 0.3422\n",
            "Epoch 30/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3742 - val_loss: 0.3411\n",
            "Epoch 31/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3731 - val_loss: 0.3403\n",
            "Epoch 32/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3720 - val_loss: 0.3396\n",
            "Epoch 33/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3710 - val_loss: 0.3386\n",
            "Epoch 34/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3700 - val_loss: 0.3377\n",
            "Epoch 35/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3691 - val_loss: 0.3368\n",
            "Epoch 36/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3681 - val_loss: 0.3360\n",
            "Epoch 37/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3673 - val_loss: 0.3354\n",
            "Epoch 38/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3664 - val_loss: 0.3348\n",
            "Epoch 39/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3656 - val_loss: 0.3341\n",
            "Epoch 40/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3648 - val_loss: 0.3335\n",
            "Epoch 41/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3641 - val_loss: 0.3329\n",
            "Epoch 42/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3633 - val_loss: 0.3324\n",
            "Epoch 43/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3626 - val_loss: 0.3317\n",
            "Epoch 44/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3620 - val_loss: 0.3313\n",
            "Epoch 45/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3613 - val_loss: 0.3308\n",
            "Epoch 46/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3607 - val_loss: 0.3309\n",
            "Epoch 47/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3601 - val_loss: 0.3305\n",
            "Epoch 48/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3595 - val_loss: 0.3304\n",
            "Epoch 49/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3589 - val_loss: 0.3302\n",
            "Epoch 50/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3584 - val_loss: 0.3299\n",
            "Epoch 51/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3579 - val_loss: 0.3302\n",
            "Epoch 52/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3574 - val_loss: 0.3297\n",
            "Epoch 53/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3569 - val_loss: 0.3294\n",
            "Epoch 54/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3564 - val_loss: 0.3291\n",
            "Epoch 55/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3559 - val_loss: 0.3286\n",
            "Epoch 56/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3554 - val_loss: 0.3283\n",
            "Epoch 57/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3550 - val_loss: 0.3278\n",
            "Epoch 58/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3545 - val_loss: 0.3274\n",
            "Epoch 59/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3541 - val_loss: 0.3271\n",
            "Epoch 60/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3536 - val_loss: 0.3268\n",
            "Epoch 61/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3532 - val_loss: 0.3264\n",
            "Epoch 62/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3527 - val_loss: 0.3259\n",
            "Epoch 63/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3523 - val_loss: 0.3256\n",
            "Epoch 64/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3518 - val_loss: 0.3253\n",
            "Epoch 65/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3514 - val_loss: 0.3248\n",
            "Epoch 66/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3510 - val_loss: 0.3243\n",
            "Epoch 67/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3506 - val_loss: 0.3238\n",
            "Epoch 68/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3502 - val_loss: 0.3233\n",
            "Epoch 69/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3498 - val_loss: 0.3228\n",
            "Epoch 70/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3495 - val_loss: 0.3222\n",
            "Epoch 71/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3491 - val_loss: 0.3218\n",
            "Epoch 72/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3487 - val_loss: 0.3213\n",
            "Epoch 73/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3483 - val_loss: 0.3208\n",
            "Epoch 74/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3480 - val_loss: 0.3204\n",
            "Epoch 75/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3476 - val_loss: 0.3199\n",
            "Epoch 76/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3472 - val_loss: 0.3196\n",
            "Epoch 77/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3469 - val_loss: 0.3192\n",
            "Epoch 78/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3465 - val_loss: 0.3189\n",
            "Epoch 79/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3462 - val_loss: 0.3186\n",
            "Epoch 80/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3458 - val_loss: 0.3183\n",
            "Epoch 81/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3455 - val_loss: 0.3181\n",
            "Epoch 82/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3452 - val_loss: 0.3179\n",
            "Epoch 83/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3449 - val_loss: 0.3177\n",
            "Epoch 84/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3446 - val_loss: 0.3173\n",
            "Epoch 85/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3443 - val_loss: 0.3173\n",
            "Epoch 86/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3440 - val_loss: 0.3169\n",
            "Epoch 87/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3437 - val_loss: 0.3169\n",
            "Epoch 88/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3434 - val_loss: 0.3164\n",
            "Epoch 89/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3431 - val_loss: 0.3163\n",
            "Epoch 90/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3428 - val_loss: 0.3161\n",
            "Epoch 91/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3425 - val_loss: 0.3158\n",
            "Epoch 92/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3421 - val_loss: 0.3156\n",
            "Epoch 93/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3418 - val_loss: 0.3154\n",
            "Epoch 94/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3415 - val_loss: 0.3153\n",
            "Epoch 95/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3412 - val_loss: 0.3149\n",
            "Epoch 96/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3409 - val_loss: 0.3148\n",
            "Epoch 97/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3406 - val_loss: 0.3145\n",
            "Epoch 98/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3404 - val_loss: 0.3144\n",
            "Epoch 99/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3401 - val_loss: 0.3143\n",
            "Epoch 100/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3398 - val_loss: 0.3138\n",
            "Epoch 101/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3395 - val_loss: 0.3136\n",
            "Epoch 102/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3392 - val_loss: 0.3134\n",
            "Epoch 103/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3390 - val_loss: 0.3133\n",
            "Epoch 104/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3387 - val_loss: 0.3133\n",
            "Epoch 105/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3384 - val_loss: 0.3128\n",
            "Epoch 106/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3382 - val_loss: 0.3128\n",
            "Epoch 107/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3379 - val_loss: 0.3125\n",
            "Epoch 108/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3376 - val_loss: 0.3123\n",
            "Epoch 109/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3374 - val_loss: 0.3123\n",
            "Epoch 110/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3371 - val_loss: 0.3118\n",
            "Epoch 111/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3369 - val_loss: 0.3117\n",
            "Epoch 112/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3366 - val_loss: 0.3114\n",
            "Epoch 113/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3364 - val_loss: 0.3115\n",
            "Epoch 114/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3362 - val_loss: 0.3111\n",
            "Epoch 115/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3359 - val_loss: 0.3108\n",
            "Epoch 116/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3357 - val_loss: 0.3106\n",
            "Epoch 117/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3354 - val_loss: 0.3104\n",
            "Epoch 118/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3352 - val_loss: 0.3104\n",
            "Epoch 119/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3349 - val_loss: 0.3100\n",
            "Epoch 120/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3347 - val_loss: 0.3099\n",
            "Epoch 121/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3344 - val_loss: 0.3097\n",
            "Epoch 122/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3341 - val_loss: 0.3097\n",
            "Epoch 123/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3339 - val_loss: 0.3093\n",
            "Epoch 124/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3337 - val_loss: 0.3091\n",
            "Epoch 125/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3334 - val_loss: 0.3088\n",
            "Epoch 126/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3332 - val_loss: 0.3086\n",
            "Epoch 127/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3329 - val_loss: 0.3084\n",
            "Epoch 128/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3327 - val_loss: 0.3082\n",
            "Epoch 129/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3324 - val_loss: 0.3081\n",
            "Epoch 130/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3321 - val_loss: 0.3077\n",
            "Epoch 131/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3319 - val_loss: 0.3076\n",
            "Epoch 132/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3316 - val_loss: 0.3073\n",
            "Epoch 133/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3314 - val_loss: 0.3072\n",
            "Epoch 134/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3311 - val_loss: 0.3070\n",
            "Epoch 135/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3309 - val_loss: 0.3068\n",
            "Epoch 136/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3307 - val_loss: 0.3066\n",
            "Epoch 137/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3304 - val_loss: 0.3064\n",
            "Epoch 138/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3302 - val_loss: 0.3061\n",
            "Epoch 139/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3300 - val_loss: 0.3062\n",
            "Epoch 140/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3298 - val_loss: 0.3060\n",
            "Epoch 141/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3295 - val_loss: 0.3064\n",
            "Epoch 142/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3293 - val_loss: 0.3065\n",
            "Epoch 143/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3291 - val_loss: 0.3066\n",
            "Epoch 144/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3288 - val_loss: 0.3053\n",
            "Epoch 145/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3286 - val_loss: 0.3052\n",
            "Epoch 146/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3284 - val_loss: 0.3050\n",
            "Epoch 147/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3282 - val_loss: 0.3049\n",
            "Epoch 148/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3279 - val_loss: 0.3048\n",
            "Epoch 149/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3277 - val_loss: 0.3044\n",
            "Epoch 150/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3275 - val_loss: 0.3046\n",
            "Epoch 151/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3273 - val_loss: 0.3037\n",
            "Epoch 152/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3271 - val_loss: 0.3043\n",
            "Epoch 153/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3269 - val_loss: 0.3033\n",
            "Epoch 154/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3266 - val_loss: 0.3053\n",
            "Epoch 155/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3264 - val_loss: 0.3033\n",
            "Epoch 156/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3262 - val_loss: 0.3053\n",
            "Epoch 157/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3260 - val_loss: 0.3031\n",
            "Epoch 158/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3258 - val_loss: 0.3070\n",
            "Epoch 159/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3256 - val_loss: 0.3030\n",
            "Epoch 160/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3253 - val_loss: 0.3062\n",
            "Epoch 161/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3251 - val_loss: 0.3020\n",
            "Epoch 162/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3249 - val_loss: 0.3052\n",
            "Epoch 163/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3247 - val_loss: 0.3014\n",
            "Epoch 164/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3245 - val_loss: 0.3070\n",
            "Epoch 165/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3243 - val_loss: 0.3010\n",
            "Epoch 166/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3240 - val_loss: 0.3076\n",
            "Epoch 167/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3238 - val_loss: 0.3003\n",
            "Epoch 168/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3237 - val_loss: 0.3059\n",
            "Epoch 169/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3234 - val_loss: 0.2999\n",
            "Epoch 170/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3232 - val_loss: 0.3063\n",
            "Epoch 171/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3230 - val_loss: 0.2998\n",
            "Epoch 172/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3228 - val_loss: 0.3061\n",
            "Epoch 173/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3226 - val_loss: 0.3000\n",
            "Epoch 174/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3224 - val_loss: 0.3087\n",
            "Epoch 175/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3222 - val_loss: 0.2997\n",
            "Epoch 176/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3220 - val_loss: 0.3116\n",
            "Epoch 177/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3218 - val_loss: 0.2998\n",
            "Epoch 178/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3216 - val_loss: 0.3127\n",
            "Epoch 179/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3214 - val_loss: 0.2998\n",
            "Epoch 180/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3212 - val_loss: 0.3173\n",
            "Epoch 181/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3210 - val_loss: 0.2999\n",
            "Epoch 182/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3208 - val_loss: 0.3166\n",
            "Epoch 183/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3205 - val_loss: 0.3006\n",
            "Epoch 184/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3203 - val_loss: 0.3207\n",
            "Epoch 185/200\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3201 - val_loss: 0.3010\n"
          ]
        }
      ],
      "source": [
        "# Treina o modelo com as callbacks definidas.\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=200,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb]\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9cc5a52b",
      "metadata": {
        "id": "9cc5a52b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebce5bde-7084-4722-ee3b-7acc495cd4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3194\n"
          ]
        }
      ],
      "source": [
        "# O melhor modelo é carregado ao final.\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e86fc2ba",
      "metadata": {
        "id": "e86fc2ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9817b958-4ec9-4c5d-d0b1-ca59b59ca282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
          ]
        }
      ],
      "source": [
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "35c86a2a",
      "metadata": {
        "id": "35c86a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49772ea4-9669-4bb5-d201-588313a3cb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: 0.477 - Predicted: 0.569\n",
            "Actual: 0.458 - Predicted: 1.428\n",
            "Actual: 5.000 - Predicted: 4.539\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(y_pred)):\n",
        "    print('Actual: %1.3f - Predicted: %1.3f' % (y_test[i], y_pred[i,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46dcefcc",
      "metadata": {
        "id": "46dcefcc"
      },
      "source": [
        "### Criando nossa própria callback.\n",
        "\n",
        "+ Se precisarmos de controle extra, podemos criar facilmente nossas próprias callbacks.\n",
        "\n",
        "\n",
        "+ Por exemplo, a seguinte callback exibirá a **época** e a **proporção entre a perda de validação e a perda de treinamento** durante o treinamento **ao final de cada época** (por exemplo, para detectarmos overfitting).\n",
        "\n",
        "\n",
        "+ Notem que a classe `PrintValTrainRatioCallback` herda da classe `keras.callbacks.Callback`.\n",
        "\n",
        "\n",
        "+ Para imprimir a razão entre a perda de validação e a perda de treinamento **ao final de cada época** devemos sobrescrever o método `on_epoch_end` da classe `Callback`.\n",
        "    + **OBS**.: para que a callback funcione, não podemos alterar a assinatura do método.\n",
        "    \n",
        "    \n",
        "+ O parâmetro `epoch` recebe o número da época corrente e o parâmetro `logs` é um dicionário com informações que incluem as perdas de treinamento e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "495043d3",
      "metadata": {
        "id": "495043d3"
      },
      "outputs": [],
      "source": [
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(\"\\nMinha callback -> epoch: {:d} - val/train: {:.4f}\".format(epoch, logs[\"val_loss\"] / logs[\"loss\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39b8d68",
      "metadata": {
        "id": "c39b8d68"
      },
      "source": [
        "### Treinando e avaliando o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "59ec5ba0",
      "metadata": {
        "id": "59ec5ba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aceb5f7f-6463-4771-9602-2ffacee2a8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m341/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3206\n",
            "Minha callback -> epoch: 0 - val/train: 1.0343\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3199 - val_loss: 0.3210\n",
            "Epoch 2/10\n",
            "\u001b[1m330/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3208\n",
            "Minha callback -> epoch: 1 - val/train: 0.9759\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3197 - val_loss: 0.3026\n",
            "Epoch 3/10\n",
            "\u001b[1m328/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3207\n",
            "Minha callback -> epoch: 2 - val/train: 1.0184\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3195 - val_loss: 0.3156\n",
            "Epoch 4/10\n",
            "\u001b[1m333/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3203\n",
            "Minha callback -> epoch: 3 - val/train: 0.9878\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3193 - val_loss: 0.3059\n",
            "Epoch 5/10\n",
            "\u001b[1m336/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3200\n",
            "Minha callback -> epoch: 4 - val/train: 1.0443\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3190 - val_loss: 0.3233\n",
            "Epoch 6/10\n",
            "\u001b[1m325/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3202\n",
            "Minha callback -> epoch: 5 - val/train: 0.9824\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3189 - val_loss: 0.3039\n",
            "Epoch 7/10\n",
            "\u001b[1m328/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3198\n",
            "Minha callback -> epoch: 6 - val/train: 1.0675\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3186 - val_loss: 0.3301\n",
            "Epoch 8/10\n",
            "\u001b[1m357/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3186\n",
            "Minha callback -> epoch: 7 - val/train: 0.9874\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3184 - val_loss: 0.3051\n",
            "Epoch 9/10\n",
            "\u001b[1m352/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3186\n",
            "Minha callback -> epoch: 8 - val/train: 1.0644\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3182 - val_loss: 0.3287\n",
            "Epoch 10/10\n",
            "\u001b[1m349/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3184\n",
            "Minha callback -> epoch: 9 - val/train: 0.9924\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3180 - val_loss: 0.3062\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3159\n"
          ]
        }
      ],
      "source": [
        "# Instancia objeto da classe PrintValTrainRatioCallback.\n",
        "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
        "\n",
        "# Treina o modelo com as callbacks definidas.\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[val_train_ratio_cb]\n",
        "                   )\n",
        "\n",
        "# O melhor modelo é carregado ao final.\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff5b30e",
      "metadata": {
        "id": "1ff5b30e"
      },
      "source": [
        "### Importante\n",
        "\n",
        "+ Podemos implementar callbacks para cada um dos seguintes eventos:\n",
        "    + on_train_begin(),\n",
        "    + on_train_end(),\n",
        "    + on_epoch_begin(),\n",
        "    + on_epoch_begin(),\n",
        "    + on_batch_end(),\n",
        "    + on_batch_end().\n",
        "\n",
        "\n",
        "+ Além disso, caso necessário, as callbacks também podem ser usadas durante a **avaliação** e a **predição**, por exemplo, para depuração. Para isso, devemos implementar alguns métodos.\n",
        "    + Os métodos abaixo são chamados pelo método **evaluate()**:\n",
        "        + on_test_begin(),\n",
        "        + on_test_end(),\n",
        "        + on_test_batch_begin(),\n",
        "        + on_test_batch_end().\n",
        "    + Os métodos abaixo são chamados pelo método **predict()**:\n",
        "        + on_predict_begin(),\n",
        "        + on_predict_end(),\n",
        "        + on_predict_batch_begin(),\n",
        "        + on_predict_batch_end()."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}